---
title: "Automatic Differentiation"
subtitle: "Julia's most confusing superpower?"
author:
  - name: Guillaume Dalle
    orcid: 0000-0003-4866-1687
    email: guillaume.dalle@epfl.ch
    affiliation: 
      - name: EPFL
      - department: IdePHICS, INDY & SPOC laboratories
  - name: Adrian Hill
    orcid: 0009-0009-5977-301X
    email: hill@tu-berlin.de
    affiliation: 
      - name: TU Berlin
      - department: Machine Learning group
date: "2024-10-29"
bibliography: AutoDiff.bib
engine: julia
format:
  revealjs:
    toc: true
    toc-depth: 1
    slide-number: true
    overview: true
    code-line-numbers: false
    scrollable: true
execute:
    echo: true
    freeze: auto
    error: true
---

# Introduction

## Motivation

::: {.callout-note}
## What is a derivative?

A linear approximation of a function around a point.
:::

::: {.callout-important}
## Why do we care?

Derivatives of complex programs are essential in optimization and machine learning.
:::

::: {.callout-tip}
## What do we need to do?

Not much: Automatic Differentiation (AD) computes derivatives for us!
:::

## Bibliography

- @blondelElementsDifferentiableProgramming2024: the most recent book
- @griewankEvaluatingDerivativesPrinciples2008: the bible of the field
- @baydinAutomaticDifferentiationMachine2018, @margossianReviewAutomaticDifferentiation2019: concise surveys

# Understanding AD

## Derivatives: formal definition

Derivative of $f$ at point $x$: linear map $\partial f(x)$ such that
$$f(x + v) = f(x) + \partial f(x)[v] + o(\lVert v \rVert)$$

In other words,

$$\partial f(x)[v] = \lim_{\varepsilon \to 0} \frac{f(x + \varepsilon v) - f(x)}{\varepsilon}$$

## Various flavors of differentiation

- **Manual**: work out $\partial f$ by hand
- **Numeric**: $\partial f(x)[v] \approx \frac{f(x+\varepsilon v) - f(x)}{\varepsilon}$
- **Symbolic**: enter a formula for $f$, get a formula for $\partial f$
- **Automatic**: code a program for $f$, get a program for $\partial f(x)$

## Two ingredients of AD

Any derivative can be obtained from:

1. Derivatives of **basic functions**: $\exp, \log, \sin, \cos, \dots$
2. Composition with the **chain rule**: 

$$\partial (f \circ g)(x) = \partial f(g(x)) \circ \partial g(x)$$

or its adjoint[^*]

$$\partial (f \circ g)^*(x) = \partial g(x)^* \circ \partial f(g(x))^*$$

[^*]: the "transpose" of a linear map

## Homemade AD (1)

Basic functions
```{julia}
double(x) = 2x
∂(::typeof(double)) = x -> (v -> 2v);  # independent from x
```


```{julia}
square(x) = x .^ 2
∂(::typeof(square)) = x -> (v -> v .* 2x);  # depends on x
```

Chain rule

```{julia}
function ∂(c::ComposedFunction)
    f, g = c.outer, c.inner
    return x -> ∂(f)(g(x)) ∘ ∂(g)(x)
end;
```

## Homemade AD (2)

Let's try it out

```{julia}
complicated_function = square ∘ double ∘ square ∘ double
x = [3, 5]
v = [1, 2];
∂(complicated_function)(x)(v)
```
```{julia}
ε = 1e-5
(complicated_function(x + ε * v) - complicated_function(x)) / ε
```

## What about Jacobian matrices?

We could multiply matrices instead of composing linear maps:

$$J_{f \circ g}(x) = J_f(g(x)) \cdot J_g(x)$$

where the Jacobian matrix is

$$J_f(x) = \left(\partial f_i / \partial x_j\right)_{i,j}$$

- very wasteful in high dimension (think of $f = \mathrm{id}$)
- ill-suited to arbitrary spaces

## Matrix-vector products

We don't need Jacobian matrices as long as we can compute their products with vectors:

:::: {.columns}

::: {.column width="50%"}
**Jacobian-vector products**

$$J_{f}(x) v = \partial f(x)[v]$$

Propagate a perturbation $v$ from input to output

:::

::: {.column width="50%"}
**Vector-Jacobian products**

$$w^\top J_{f}(x) = \partial f(x)^*[w]$$

Backpropagate a sensitivity $w$ from output to input

:::

::::

## Forward mode

Consider $f = f_L \circ \dots \circ f_1$ and its Jacobian $J = J_L \cdots J_1$.

Jacobian-vector products decompose from layer $1$ to layer $L$:

$$J_L(J_{L-1}(\dots \underbrace{J_2(\underbrace{J_1 v}_{v_1})}_{v_2}))$$

Forward mode AD relies on the chain rule.

## Reverse mode

Consider $f = f_L \circ \dots \circ f_1$ and its Jacobian $J = J_L \cdots J_1$.

Vector-Jacobian products decompose from layer $L$ to layer $1$:

$$((\underbrace{(\underbrace{w^\top J_L}_{w_L}) J_{L-1}}_{w_{L-1}} \dots ) J_2 ) J_1$$

Reverse mode AD relies on the adjoint chain rule.

## Computational complexity

Consider $f : \mathbb{R}^n \to \mathbb{R}^m$. How much does its Jacobian cost?

::: {.callout-important}
## Theorem

Each JVP or VJP takes as much time and space as $O(1)$ calls to $f$.
:::

:::: {.columns}

::: {.column width="50%"}
**Forward mode**

Column by column: $O(n)$

$$J = \begin{pmatrix} J e_1 & \dots & J e_n \end{pmatrix}$$

:::

::: {.column width="50%"}
**Reverse mode**

Row by row: $O(m)$

$$J = \begin{pmatrix} e_1^\top J \\ \dots \\ e_m^\top J \end{pmatrix}$$

:::

::::

# Using AD

## Three types of AD users

1. **Package users** want to differentiate through functions
2. **Package developers** want to write differentiable functions
3. **Backend developers** want to create new AD systems

## Python vs. Julia: users {.smaller}

![](img/python_julia_user.png)

Image: courtesy of Adrian Hill

## Python vs. Julia: developers {.smaller}

![](img/python_julia_dev.png)

Image: courtesy of Adrian Hill

## Why so many backends?

- Conflicting **paradigms**:
  - numeric vs. symbolic vs. algorithmic
  - operator overloading vs.  source-to-source
- Cover varying **subsets of the language**
- Historical reasons: developed by **different people**

Exhaustive list available at <https://juliadiff.org/>.

## Meaningful criteria

- Does this AD backend **execute** **without error**?
- Does it return the **right derivative**?
- Does it run **fast enough** for me?

## A simple decision tree

1. **Follow recommendations** of high-level library (e.g. Flux).
2. Otherwise, **choose mode** from input/output dimensions.
3. Try the most **battle-tested** backends:
   - [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl) or [Enzyme](https://github.com/EnzymeAD/Enzyme.jl) in forward mode,
   - [Zygote](https://github.com/FluxML/Zygote.jl) or [Enzyme](https://github.com/EnzymeAD/Enzyme.jl) in reverse mode.
4. If nothing works, fall back on finite differences.

# Enabling AD

## Each backend has demands

- ForwardDiff: generic number types
- Zygote: no mutation
- Enzyme: correct annotations, type stability

## Typical ForwardDiff misuse

```{julia}
import ForwardDiff

badcopy(x) = copyto!(zeros(size(x)), x)

ForwardDiff.jacobian(badcopy, ones(2))
```

## ForwardDiff troubleshooting

Allow numbers of [type `Dual`](https://juliadiff.org/ForwardDiff.jl/stable/dev/how_it_works/) to pass through your functions.

```{julia}
goodcopy(x) = copyto!(zeros(eltype(x), size(x)), x)

ForwardDiff.jacobian(goodcopy, ones(2))
```

## Typical Zygote misuse

```{julia}
import Zygote

Zygote.jacobian(badcopy, ones(2))
```

## Zygote troubleshooting

Define a [custom rule](https://juliadiff.org/ChainRulesCore.jl/stable/) with [ChainRulesCore](https://github.com/JuliaDiff/ChainRulesCore.jl):

```{julia}
using ChainRulesCore, LinearAlgebra

badcopy2(x) = badcopy(x)

function ChainRulesCore.rrule(::typeof(badcopy2), x)
    y = badcopy2(x)
    function badcopy2_pullback(dy)
        df, dx = NoTangent(), I' * dy  # VJP
        return (df, dx)
    end
    return y, badcopy2_pullback
end

Zygote.jacobian(badcopy2, ones(2))
```

## Typical Enzyme misuse 

```{julia}
import Enzyme

Enzyme.autodiff(
  Enzyme.Forward,
  badcopy,
  Enzyme.Active(ones(2))
)
```

## Enzyme troubleshooting

Pay attention to activity annotations, type stability and temporary storage (see the [FAQ](https://enzymead.github.io/Enzyme.jl/stable/faq/)).

```julia
Enzyme.autodiff(
  Enzyme.Forward,
  badcopy,
  Enzyme.Duplicated(ones(2), zeros(2))
)
```

# DifferentiationInterface

## Goals

- [DifferentiationInterface](https://github.com/gdalle/DifferentiationInterface.jl) (DI) offers a **common syntax** for all AD backends^[inspired by [AbstractDifferentiation](https://github.com/JuliaDiff/AbstractDifferentiation.jl)]
- AD users can compare correctness and performance **without reading each documentation**
- AD developers get access to a wider user base

::: {.callout-warning}
## The fine print

DI may be slower than a direct call to the backend's API (mostly with Enzyme).
:::

## Supported packages

<!-- Using columns to fit everything on one slide -->
:::: {.columns}

::: {.column width="50%"}
* [ChainRulesCore](https://github.com/JuliaDiff/ChainRulesCore.jl)
* [Diffractor](https://github.com/JuliaDiff/Diffractor.jl)
* [Enzyme](https://github.com/EnzymeAD/Enzyme.jl)
* [FastDifferentiation](https://github.com/brianguenter/FastDifferentiation.jl)
* [FiniteDiff](https://github.com/JuliaDiff/FiniteDiff.jl)
* [FiniteDifferences](https://github.com/JuliaDiff/FiniteDifferences.jl)
* [ForwardDiff](https://github.com/JuliaDiff/ForwardDiff.jl)
:::

::: {.column width="50%"}
* [PolyesterForwardDiff](https://github.com/JuliaDiff/PolyesterForwardDiff.jl)
* [ReverseDiff](https://github.com/JuliaDiff/ReverseDiff.jl)
* [Symbolics](https://github.com/JuliaSymbolics/Symbolics.jl)
* [Mooncake](https://github.com/compintell/Mooncake.jl)
* [Tracker](https://github.com/FluxML/Tracker.jl)
* [Zygote](https://github.com/FluxML/Zygote.jl)
:::

::::

## Getting started

1. Load the necessary packages
```{julia}
#| output: false
using DifferentiationInterface; import ForwardDiff, Enzyme, Zygote
f(x) = sum(abs2, x)
x = [1.0, 2.0, 3.0, 4.0]
```

2. Combine one of DI's operators with a backend from [ADTypes](https://github.com/SciML/ADTypes.jl)

```{julia}
value_and_gradient(f, AutoForwardDiff(), x)
```
```{julia}
value_and_gradient(f, AutoEnzyme(), x)
```
```{julia}
value_and_gradient(f, AutoZygote(), x)
```

3. Increase performance via DI's preparation mechanism.

## Features

- Support for functions `f(x)` or `f!(y, x)` with **scalar/array inputs & outputs**
- Eight standard **operators**: `pushforward`, `pullback`, `derivative`, `gradient`, `jacobian`, `hvp`, `second_derivative`, `hessian`
- Out-of-place and **in-place** versions
- **Combine** different backends using `SecondOrder`
- **Translate** between backends using `DifferentiateWith`

# Sparsity

# Conclusion

## Going further

- [x] AD through a simple function
- [ ] AD through an expectation [@mohamedMonteCarloGradient2020]
- [ ] AD through a convex solver [@blondelEfficientModularImplicit2022]
- [ ] AD through a combinatorial solver [@mandiDecisionFocusedLearningFoundations2023]

## Take-home message

**Computing derivatives is easy**, but each AD solution comes with its own **limitations**.

Learn to recognize and overcome them, either as a user or as a developer.

## References

::: {#refs}
:::